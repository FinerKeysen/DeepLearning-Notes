来自原文["机器学习算法如何调参？这里有一份神经网络学习速率设置指南"SDGR节选](https://www.jiqizhixin.com/articles/nn-learning-rate)

**带有热重启的随机梯度下降（SGDR）**

带有热重启的随机梯度下降（SGDR）与周期性方法很相似，其中一个积极的退火表与周期性「再启动」融合到原始的初始学习率之中。
![](https://image.jiqizhixin.com/uploads/editor/af3eed9b-4a6c-4e21-9fa6-de2dd699f97b/640.jpeg)

我们可以将其写为
![](https://image.jiqizhixin.com/uploads/editor/59a72dbc-0b71-49cc-90d0-2249298a9c4e/640.png)

其中![](http://latex.codecogs.com/gif.latex?\{\eta_t}) 是时间步 t 的学习率，（在每一个 mini batch 间增长）![](http://latex.codecogs.com/gif.latex?\{\eta^i_{max}})和![](http://latex.codecogs.com/gif.latex?\{\eta^i_{min}})定义理想学习率的范围，![](http://latex.codecogs.com/gif.latse?\{T_{current}}) 表征上次再启动之后 epoch 的数量，![](http://latex.codecogs.com/gif.latex?\{T_i}) 定义周期之中 epoch 的数量。让我们试着分解这个等式。

这个退火表依赖于余弦函数，其在-1 和 1 之间变化。![](http://latex.codecogs.com/gif.latex?\{\frac{T_{current}}{T_i}})能够取 0 到 1 之间的值，这是我们的余弦函数的输入。余弦函数的相应区域在下图用绿色突出显示。通过添加 1，我们的函数在 0 到 2 之间变化，然后缩小 1/2，现在在 0 到 1 之间变化。因此，我们简单地取极小值学习率，并添加指定学习率范围的一部分。由于这一函数从 1 开始并降为 0，结果是一个从特定范围的极大值开始并衰减为极小值的学习率。一旦我们的周期结束，![](http://latex.codecogs.com/gif.latex?\{T_{current}}) 重置为 0，我们从极大值学习率再开始循环这一过程。
![](https://image.jiqizhixin.com/uploads/editor/e180aa64-62e6-429f-afc2-fdb5e92bf078/640.png)

作者也发现这个学习速率安排表可以适用于：
 1.当训练进行时延长周期
 2.在每一周期之后衰减 ![](http://latex.codecogs.com/gif.latex?\{\eta^i_{max}})和![](http://latex.codecogs.com/gif.latex?\{\eta^i_{min}})

在每一次重启的时候彻底地提高学习速率，我们可以本质上的退出一个局部低点并且继续探索损失地图。

非常酷的主意：在每一轮循环后截图一下权重，研究员可以通过训练单个模型去建立一个全套模型。这是因为从一个周期到另一个周期，这个网络「沉淀」在不同的局部最优，像在下面图中画的一样。
![](https://image.jiqizhixin.com/uploads/editor/ee47c171-903c-4dc8-b438-e514b43f6d01/640.jpeg)


这里给出 `SGDR`部分的代码实现，该片段来自keras的[callbacks.py](https://github.com/nebw/keras/blob/eafe45c410b3ca8699ba00afe0ba50a36cbc7cac/keras/callbacks.py)

简单实现参考[SGDR查找最优LR的简单实现.py](https://github.com/FinerKeysen/blogs/blob/master/LearningRate/Keras/SGDR%E6%9F%A5%E6%89%BE%E6%9C%80%E4%BC%98LR%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0.py)

```python
#*************************************************#
#                SGDR的Keras实现                   #
#*************************************************#
class SGDRScheduler(Callback):
    '''Schedule learning rates with restarts
    A simple restart technique for stochastic gradient descent.
    The learning rate decays after each batch and peridically resets to its
    initial value. Optionally, the learning rate is additionally reduced by a
    fixed factor at a predifined set of epochs.
    # Arguments
        epochsize: Number of samples per epoch during training.
        batchsize: Number of samples per batch during training.
        start_epoch: First epoch where decay is applied.
        epochs_to_restart: Initial number of epochs before restarts.
        mult_factor: Increase of epochs_to_restart after each restart.
        lr_fac: Decrease of learning rate at epochs given in
                lr_reduction_epochs.
        lr_reduction_epochs: Fixed list of epochs at which to reduce
                             learning rate.
    # References
        - [SGDR: Stochastic Gradient Descent with Restarts](http://arxiv.org/abs/1608.03983)
    '''
    def __init__(self,
                 epochsize,
                 batchsize,
                 epochs_to_restart=2,
                 mult_factor=2,
                 lr_fac=0.1,
                 lr_reduction_epochs=(60, 120, 160)):
        super(SGDRScheduler, self).__init__()
        self.epoch = -1
        self.batch_since_restart = 0
        self.next_restart = epochs_to_restart
        self.epochsize = epochsize
        self.batchsize = batchsize
        self.epochs_to_restart = epochs_to_restart
        self.mult_factor = mult_factor
        self.batches_per_epoch = self.epochsize / self.batchsize
        self.lr_fac = lr_fac
        self.lr_reduction_epochs = lr_reduction_epochs
        self.lr_log = []

    def on_train_begin(self, logs={}):
        self.lr = K.get_value(self.model.optimizer.lr)

    def on_epoch_begin(self, epoch, logs={}):
        self.epoch += 1

    def on_batch_end(self, batch, logs={}):
        fraction_to_restart = self.batch_since_restart / \
            (self.batches_per_epoch * self.epochs_to_restart)
        lr = 0.5 * self.lr * (1 + np.cos(fraction_to_restart * np.pi))
        K.set_value(self.model.optimizer.lr, lr)

        self.batch_since_restart += 1
        self.lr_log.append(lr)

    def on_epoch_end(self, epoch, logs={}):
        if self.epoch + 1 == self.next_restart:
            self.batch_since_restart = 0
            self.epochs_to_restart *= self.mult_factor
            self.next_restart += self.epochs_to_restart

        if (self.epoch + 1) in self.lr_reduction_epochs:
self.lr *= self.lr_fac
```


